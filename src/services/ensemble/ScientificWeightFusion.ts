/**
 * ‚öñÔ∏è SCIENTIFIC WEIGHT FUSION - FUS√ÉO BAYESIANA CIENT√çFICA 2025
 * 
 * Baseado em pesquisas cient√≠ficas:
 * - Bayesian Model Averaging (Hoeting et al., 1999)
 * - Variational Bayesian Inference (Blei et al., 2017)
 * - Adaptive Gaussian Process Priors (Rasmussen & Williams, 2023)
 * - Evidence-Based Weight Combination (Modern Ensemble Learning 2025)
 */

import type { DynamicWeight, AlgorithmAccuracy } from '../../types/ensemble.types'

export interface BayesianPrior {
  algorithmId: string
  priorMean: number
  priorVariance: number
  observationCount: number
  evidenceStrength: number
}

export interface WeightUncertainty {
  algorithmId: string
  weightMean: number
  weightVariance: number
  confidenceInterval: [number, number]
  uncertaintyLevel: number
}

export interface FusionMetrics {
  totalEvidence: number
  weightEntropy: number
  diversityIndex: number
  bayesianCoherence: number
  fusionConfidence: number
}

export class ScientificWeightFusion {
  private bayesianPriors: Map<string, BayesianPrior> = new Map()
  private weightHistory: Map<string, number[]> = new Map()
  private evidenceAccumulation: Map<string, number> = new Map()
  
  // Hyper-parameters baseados em pesquisas
  private priorStrength: number = 2.0 // Beta distribution concentration
  private evidenceDecay: number = 0.95 // Evidence decay factor
  private uncertaintyThreshold: number = 0.1 // Maximum acceptable uncertainty
  
  /**
   * ‚öñÔ∏è FUS√ÉO CIENT√çFICA PRINCIPAL
   * Combina pesos usando infer√™ncia bayesiana
   */
  fuseBayesianWeights(
    weights: Map<string, DynamicWeight>,
    accuracies: Map<string, AlgorithmAccuracy>
  ): Map<string, DynamicWeight> {
    
    const fusedWeights = new Map<string, DynamicWeight>()
    
    // üßÆ CALCULAR EVID√äNCIA BAYESIANA para cada algoritmo
    const evidenceMap = this.calculateBayesianEvidence(weights, accuracies)
    
    // üéØ INFER√äNCIA VARIACIONAL para fus√£o √≥tima
    const posteriorWeights = this.variationalInference(weights, evidenceMap)
    
    // ‚öñÔ∏è APLICAR GAUSSIAN PROCESS PRIORS
    const smoothedWeights = this.applyGaussianProcessPriors(posteriorWeights)
    
    // üìä COMBINAR TUDO com uncertainty quantification
    for (const [algorithmId, weight] of weights) {
      const fusedWeight = this.fuseIndividualWeight(
        algorithmId,
        weight,
        evidenceMap.get(algorithmId) || 0,
        smoothedWeights.get(algorithmId) || weight.final_weight
      )
      
      fusedWeights.set(algorithmId, fusedWeight)
    }
    
    // üéõÔ∏è NORMALIZA√á√ÉO CIENT√çFICA final
    return this.scientificNormalization(fusedWeights)
  }
  
  /**
   * üßÆ EVID√äNCIA BAYESIANA por algoritmo
   */
  private calculateBayesianEvidence(
    weights: Map<string, DynamicWeight>,
    accuracies: Map<string, AlgorithmAccuracy>
  ): Map<string, number> {
    
    const evidenceMap = new Map<string, number>()
    
    for (const [algorithmId, accuracy] of accuracies) {
      // üìä LIKELIHOOD baseado em performance
      const likelihood = this.calculateLikelihood(accuracy)
      
      // üéØ PRIOR baseado em hist√≥rico
      const prior = this.getBayesianPrior(algorithmId)
      
      // üßÆ MARGINAL LIKELIHOOD (evid√™ncia)
      const marginalLikelihood = this.calculateMarginalLikelihood(likelihood, prior)
      
      // ‚öñÔ∏è POSTERIOR EVIDENCE usando Bayes theorem
      const evidence = (likelihood * prior.evidenceStrength) / (marginalLikelihood + 1e-8)
      
      evidenceMap.set(algorithmId, Math.max(0.01, Math.min(10.0, evidence)))
      
      // üìà ATUALIZAR PRIOR com nova evid√™ncia
      this.updateBayesianPrior(algorithmId, likelihood, evidence)
    }
    
    return evidenceMap
  }
  
  /**
   * üìä LIKELIHOOD baseado em performance
   */
  private calculateLikelihood(accuracy: AlgorithmAccuracy): number {
    // Gaussian likelihood com emphasis em consist√™ncia
    const meanAccuracy = accuracy.accuracy_rate
    const recentAccuracy = accuracy.last_accuracy
    
    // Penalizar inconsist√™ncia (alta vari√¢ncia)
    const variance = Math.abs(meanAccuracy - recentAccuracy)
    const consistency = Math.exp(-variance * 5) // Penalty for inconsistency
    
    // Beta distribution likelihood para accuracy
    const alpha = meanAccuracy * 10 + 1
    const beta = (1 - meanAccuracy) * 10 + 1
    const betaLikelihood = this.betaPDF(recentAccuracy, alpha, beta)
    
    return betaLikelihood * consistency
  }
  
  /**
   * üéØ MARGINAL LIKELIHOOD (evid√™ncia total)
   */
  private calculateMarginalLikelihood(likelihood: number, prior: BayesianPrior): number {
    // Aproxima√ß√£o usando Laplace approximation
    const evidenceContribution = likelihood * Math.sqrt(2 * Math.PI * prior.priorVariance)
    return evidenceContribution * Math.exp(-0.5 * Math.pow(likelihood - prior.priorMean, 2) / prior.priorVariance)
  }
  
  /**
   * üéõÔ∏è INFER√äNCIA VARIACIONAL
   */
  private variationalInference(
    weights: Map<string, DynamicWeight>,
    evidence: Map<string, number>
  ): Map<string, number> {
    
    const posteriorWeights = new Map<string, number>()
    let totalEvidence = 0
    
    // Calcular evid√™ncia total para normaliza√ß√£o
    for (const evidenceValue of evidence.values()) {
      totalEvidence += evidenceValue
    }
    
    // Infer√™ncia variacional usando mean-field approximation
    for (const [algorithmId, weight] of weights) {
      const algo_evidence = evidence.get(algorithmId) || 0.01
      const prior = this.getBayesianPrior(algorithmId)
      
      // Variational posterior parameters
      const posteriorPrecision = 1 / prior.priorVariance + algo_evidence
      const posteriorMean = (prior.priorMean / prior.priorVariance + weight.final_weight * algo_evidence) / posteriorPrecision
      
      // Suaviza√ß√£o com uncertainty
      const uncertainty = 1 / Math.sqrt(posteriorPrecision)
      const confidenceWeight = Math.exp(-uncertainty)
      
      const finalPosterior = posteriorMean * confidenceWeight + weight.final_weight * (1 - confidenceWeight)
      
      posteriorWeights.set(algorithmId, Math.max(0.01, Math.min(1.0, finalPosterior)))
    }
    
    return posteriorWeights
  }
  
  /**
   * üåä GAUSSIAN PROCESS PRIORS
   */
  private applyGaussianProcessPriors(weights: Map<string, number>): Map<string, number> {
    const smoothedWeights = new Map<string, number>()
    
    // Kernel de suaviza√ß√£o temporal (RBF kernel)
    const kernelWidth = 0.2
    
    for (const [algorithmId, weight] of weights) {
      const history = this.weightHistory.get(algorithmId) || []
      
      if (history.length < 3) {
        // Insufficient history, use current weight
        smoothedWeights.set(algorithmId, weight)
        continue
      }
      
      // GP smoothing usando √∫ltimas observa√ß√µes
      let weightedSum = 0
      let kernelSum = 0
      
      for (let i = 0; i < Math.min(history.length, 10); i++) {
        const timeDiff = i / history.length // Normalized time difference
        const kernelValue = Math.exp(-Math.pow(timeDiff, 2) / (2 * kernelWidth * kernelWidth))
        
        weightedSum += history[history.length - 1 - i] * kernelValue
        kernelSum += kernelValue
      }
      
      const gpSmoothed = kernelSum > 0 ? weightedSum / kernelSum : weight
      
      // Combine with current weight
      const alpha = 0.7 // GP influence
      const finalWeight = alpha * gpSmoothed + (1 - alpha) * weight
      
      smoothedWeights.set(algorithmId, finalWeight)
    }
    
    return smoothedWeights
  }
  
  /**
   * ‚öñÔ∏è FUS√ÉO INDIVIDUAL por algoritmo
   */
  private fuseIndividualWeight(
    algorithmId: string,
    originalWeight: DynamicWeight,
    evidence: number,
    posteriorWeight: number
  ): DynamicWeight {
    
    // üìä CALCULAR UNCERTAINTY
    const uncertainty = this.calculateWeightUncertainty(algorithmId, evidence)
    
    // ‚öñÔ∏è FUS√ÉO CONSERVADORA quando uncertainty alta
    const conservativeFactor = uncertainty.uncertaintyLevel > this.uncertaintyThreshold ? 0.5 : 1.0
    
    // üéØ PESO FINAL cient√≠fico
    const scientificWeight = posteriorWeight * conservativeFactor
    
    // üìà ATUALIZAR HIST√ìRICO
    this.updateWeightHistory(algorithmId, scientificWeight)
    
    return {
      ...originalWeight,
      final_weight: Math.max(0.01, Math.min(1.0, scientificWeight)),
      confidence_level: 1 - uncertainty.uncertaintyLevel,
      weight_source: 'hybrid', // Bayesian fusion
      last_updated: Date.now()
    }
  }
  
  /**
   * üìä INCERTEZA DO PESO
   */
  private calculateWeightUncertainty(algorithmId: string, evidence: number): WeightUncertainty {
    const prior = this.getBayesianPrior(algorithmId)
    const history = this.weightHistory.get(algorithmId) || []
    
    // Variance baseada em evid√™ncia
    const evidenceVariance = 1 / (evidence + prior.evidenceStrength)
    
    // Variance hist√≥rica
    const historicalVariance = history.length > 1 ? this.calculateVariance(history) : 0.1
    
    // Combinar vari√¢ncias
    const totalVariance = 0.7 * evidenceVariance + 0.3 * historicalVariance
    const uncertaintyLevel = Math.sqrt(totalVariance)
    
    const weightMean = history.length > 0 ? history[history.length - 1] : 0.5
    const confidenceInterval: [number, number] = [
      Math.max(0, weightMean - 1.96 * Math.sqrt(totalVariance)),
      Math.min(1, weightMean + 1.96 * Math.sqrt(totalVariance))
    ]
    
    return {
      algorithmId,
      weightMean,
      weightVariance: totalVariance,
      confidenceInterval,
      uncertaintyLevel: Math.min(0.5, uncertaintyLevel)
    }
  }
  
  /**
   * üéõÔ∏è NORMALIZA√á√ÉO CIENT√çFICA
   */
  private scientificNormalization(weights: Map<string, DynamicWeight>): Map<string, DynamicWeight> {
    // Normaliza√ß√£o conservadora que preserva ratios importantes
    
    let totalWeight = 0
    let maxWeight = 0
    
    for (const weight of weights.values()) {
      totalWeight += weight.final_weight
      maxWeight = Math.max(maxWeight, weight.final_weight)
    }
    
    // Evitar over-normalization
    const normalizationFactor = totalWeight > 0 ? Math.min(1.5, 1.0 / totalWeight) : 1.0
    
    const normalizedWeights = new Map<string, DynamicWeight>()
    
    for (const [algorithmId, weight] of weights) {
      const normalizedWeight = weight.final_weight * normalizationFactor
      
      // Preservar pesos relativamente altos
      const preservationFactor = weight.final_weight === maxWeight ? 1.1 : 1.0
      
      const finalWeight = Math.max(0.01, Math.min(0.8, normalizedWeight * preservationFactor))
      
      normalizedWeights.set(algorithmId, {
        ...weight,
        final_weight: finalWeight
      })
    }
    
    return normalizedWeights
  }
  
  /**
   * üéØ PRIOR BAYESIANO por algoritmo
   */
  private getBayesianPrior(algorithmId: string): BayesianPrior {
    if (!this.bayesianPriors.has(algorithmId)) {
      // Prior n√£o-informativo (vago)
      this.bayesianPriors.set(algorithmId, {
        algorithmId,
        priorMean: 0.5, // Neutral prior
        priorVariance: 0.25, // Moderate uncertainty
        observationCount: 1,
        evidenceStrength: this.priorStrength
      })
    }
    
    return this.bayesianPriors.get(algorithmId)!
  }
  
  /**
   * üìà ATUALIZAR PRIOR bayesiano
   */
  private updateBayesianPrior(algorithmId: string, likelihood: number, evidence: number): void {
    const prior = this.getBayesianPrior(algorithmId)
    
    // Atualiza√ß√£o conjugada (aproxima√ß√£o)
    const alpha = 0.1 // Taxa de aprendizado
    prior.priorMean = alpha * likelihood + (1 - alpha) * prior.priorMean
    prior.observationCount += 1
    prior.evidenceStrength = Math.min(10.0, prior.evidenceStrength + evidence * 0.1)
    
    // Reduzir vari√¢ncia com mais observa√ß√µes
    prior.priorVariance = Math.max(0.01, prior.priorVariance * this.evidenceDecay)
  }
  
  /**
   * üìà ATUALIZAR HIST√ìRICO de pesos
   */
  private updateWeightHistory(algorithmId: string, weight: number): void {
    if (!this.weightHistory.has(algorithmId)) {
      this.weightHistory.set(algorithmId, [])
    }
    
    const history = this.weightHistory.get(algorithmId)!
    history.push(weight)
    
    // Manter apenas √∫ltimas 50 observa√ß√µes
    if (history.length > 50) {
      history.shift()
    }
  }
  
  /**
   * üìä VARI√ÇNCIA de array
   */
  private calculateVariance(data: number[]): number {
    if (data.length < 2) return 0.1
    
    const mean = data.reduce((sum, x) => sum + x, 0) / data.length
    const variance = data.reduce((sum, x) => sum + Math.pow(x - mean, 2), 0) / (data.length - 1)
    
    return Math.max(0.01, variance)
  }
  
  /**
   * üìä BETA PDF
   */
  private betaPDF(x: number, alpha: number, beta: number): number {
    // Simplified beta PDF (without normalization constant for efficiency)
    if (x <= 0 || x >= 1) return 0.001
    
    return Math.pow(x, alpha - 1) * Math.pow(1 - x, beta - 1)
  }
  
  /**
   * üìä M√âTRICAS DE FUS√ÉO
   */
  calculateFusionMetrics(weights: Map<string, DynamicWeight>): FusionMetrics {
    let totalEvidence = 0
    let weightEntropy = 0
    let maxWeight = 0
    
    for (const weight of weights.values()) {
      const w = weight.final_weight
      totalEvidence += w
      maxWeight = Math.max(maxWeight, w)
      
      // Shannon entropy
      if (w > 0.001) {
        weightEntropy -= w * Math.log2(w)
      }
    }
    
    // Diversity index (1 - Herfindahl index)
    let herfindahl = 0
    for (const weight of weights.values()) {
      const normalizedWeight = weight.final_weight / totalEvidence
      herfindahl += normalizedWeight * normalizedWeight
    }
    const diversityIndex = 1 - herfindahl
    
    // Bayesian coherence (higher = more coherent with priors)
    const bayesianCoherence = this.calculateBayesianCoherence(weights)
    
    // Fusion confidence (lower entropy = higher confidence)
    const maxEntropy = Math.log2(weights.size)
    const fusionConfidence = 1 - (weightEntropy / maxEntropy)
    
    return {
      totalEvidence,
      weightEntropy,
      diversityIndex,
      bayesianCoherence,
      fusionConfidence
    }
  }
  
  /**
   * üéØ COER√äNCIA BAYESIANA
   */
  private calculateBayesianCoherence(weights: Map<string, DynamicWeight>): number {
    let coherenceSum = 0
    let count = 0
    
    for (const [algorithmId, weight] of weights) {
      const prior = this.getBayesianPrior(algorithmId)
      const deviation = Math.abs(weight.final_weight - prior.priorMean)
      const expectedDeviation = Math.sqrt(prior.priorVariance)
      
      const coherence = Math.exp(-deviation / expectedDeviation)
      coherenceSum += coherence
      count++
    }
    
    return count > 0 ? coherenceSum / count : 0.5
  }
  
  /**
   * üéõÔ∏è CONFIGURAR HIPERPAR√ÇMETROS
   */
  setHyperParameters(
    priorStrength: number,
    evidenceDecay: number,
    uncertaintyThreshold: number
  ): void {
    this.priorStrength = Math.max(0.5, Math.min(10.0, priorStrength))
    this.evidenceDecay = Math.max(0.8, Math.min(0.99, evidenceDecay))
    this.uncertaintyThreshold = Math.max(0.05, Math.min(0.3, uncertaintyThreshold))
    
    console.log(`‚öñÔ∏è Hiperpar√¢metros bayesianos atualizados: prior=${this.priorStrength}, decay=${this.evidenceDecay}, uncertainty=${this.uncertaintyThreshold}`)
  }
  
  /**
   * üìä RELAT√ìRIO DE FUS√ÉO
   */
  generateFusionReport(weights: Map<string, DynamicWeight>): string {
    const metrics = this.calculateFusionMetrics(weights)
    
    let report = `\n‚öñÔ∏è RELAT√ìRIO DE FUS√ÉO BAYESIANA:\n`
    report += `   üìä Evid√™ncia Total: ${metrics.totalEvidence.toFixed(3)}\n`
    report += `   üåä Entropia de Pesos: ${metrics.weightEntropy.toFixed(3)}\n`
    report += `   üé≠ √çndice de Diversidade: ${metrics.diversityIndex.toFixed(3)}\n`
    report += `   üéØ Coer√™ncia Bayesiana: ${metrics.bayesianCoherence.toFixed(3)}\n`
    report += `   üîí Confian√ßa da Fus√£o: ${(metrics.fusionConfidence * 100).toFixed(1)}%\n\n`
    
    report += `üéõÔ∏è PESOS FUNDIDOS POR ALGORITMO:\n`
    for (const [algorithmId, weight] of weights) {
      const uncertainty = this.calculateWeightUncertainty(algorithmId, weight.final_weight)
      report += `   ${algorithmId}: ${(weight.final_weight * 100).toFixed(1)}% (¬±${(uncertainty.uncertaintyLevel * 100).toFixed(1)}%)\n`
    }
    
    return report
  }
} 